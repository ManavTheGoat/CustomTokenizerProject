{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":145674,"sourceType":"modelInstanceVersion","modelInstanceId":123526,"modelId":146585}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import logging\nfrom datasets import load_dataset\nfrom transformers import AutoTokenizer\nfrom tokenizers import Tokenizer, models, trainers, normalizers, pre_tokenizers, decoders, processors\nfrom transformers import BertTokenizerFast, GPT2TokenizerFast, AlbertTokenizerFast","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-12-16T16:27:24.891362Z","iopub.execute_input":"2024-12-16T16:27:24.891626Z","iopub.status.idle":"2024-12-16T16:27:30.992586Z","shell.execute_reply.started":"2024-12-16T16:27:24.891591Z","shell.execute_reply":"2024-12-16T16:27:30.991878Z"},"trusted":true},"outputs":[],"execution_count":1},{"cell_type":"code","source":"logging.basicConfig(level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-16T16:27:52.922875Z","iopub.execute_input":"2024-12-16T16:27:52.923546Z","iopub.status.idle":"2024-12-16T16:27:52.927380Z","shell.execute_reply.started":"2024-12-16T16:27:52.923515Z","shell.execute_reply":"2024-12-16T16:27:52.926511Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"# Configurable parameters\nCONFIG = {\n    \"dataset_name\": \"wikitext\",\n    \"dataset_config\": \"wikitext-2-raw-v1\",\n    \"split\": \"train\",\n    \"vocab_size\": 25000,\n    \"batch_size\": 1000,\n    \"special_tokens\": [\"[UNK]\", \"[PAD]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"],\n}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-16T16:28:29.220708Z","iopub.execute_input":"2024-12-16T16:28:29.221044Z","iopub.status.idle":"2024-12-16T16:28:29.225601Z","shell.execute_reply.started":"2024-12-16T16:28:29.221015Z","shell.execute_reply":"2024-12-16T16:28:29.224757Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"def load_data(config):\n    logging.info(\"Loading dataset...\")\n    dataset = load_dataset(config[\"dataset_name\"], name=config[\"dataset_config\"], split=config[\"split\"])\n    logging.info(f\"Loaded dataset with {len(dataset)} samples.\")\n    return dataset\n\ndef batch_iterator(dataset, batch_size):\n    for i in range(0, len(dataset), batch_size):\n        yield dataset[i: i + batch_size][\"text\"]\n\ndef train_tokenizer_with_model(model_type, dataset, config):\n    if model_type == \"bert\":\n        tokenizer = Tokenizer(models.WordPiece(unk_token=\"[UNK]\"))\n        tokenizer.normalizer = normalizers.BertNormalizer(lowercase=True)\n        tokenizer.pre_tokenizer = pre_tokenizers.BertPreTokenizer()\n        trainer = trainers.WordPieceTrainer(vocab_size=config[\"vocab_size\"], special_tokens=config[\"special_tokens\"])\n        tokenizer.train_from_iterator(batch_iterator(dataset, config[\"batch_size\"]), trainer=trainer)\n\n        # Post-processing\n        cls_id = tokenizer.token_to_id(\"[CLS]\")\n        sep_id = tokenizer.token_to_id(\"[SEP]\")\n        tokenizer.post_processor = processors.TemplateProcessing(\n            single=\"[CLS]:0 $A:0 [SEP]:0\",\n            pair=\"[CLS]:0 $A:0 [SEP]:0 $B:1 [SEP]:1\",\n            special_tokens=[(\"[CLS]\", cls_id), (\"[SEP]\", sep_id)],\n        )\n        tokenizer.decoder = decoders.WordPiece(prefix=\"##\")\n        tokenizer = BertTokenizerFast(tokenizer_object=tokenizer)\n\n    elif model_type == \"gpt2\":\n        tokenizer = Tokenizer(models.BPE())\n        tokenizer.pre_tokenizer = pre_tokenizers.ByteLevel(add_prefix_space=False)\n        trainer = trainers.BpeTrainer(vocab_size=config[\"vocab_size\"], special_tokens=[\"<|endoftext|>\"])\n        tokenizer.train_from_iterator(batch_iterator(dataset, config[\"batch_size\"]), trainer=trainer)\n        tokenizer.post_processor = processors.ByteLevel(trim_offsets=False)\n        tokenizer.decoder = decoders.ByteLevel()\n        tokenizer = GPT2TokenizerFast(tokenizer_object=tokenizer)\n\n    elif model_type == \"t5\":\n        tokenizer = Tokenizer(models.Unigram())\n        tokenizer.normalizer = normalizers.Sequence([\n            normalizers.Replace(\"``\", '\"'), \n            normalizers.Replace(\"''\", '\"'), \n            normalizers.Lowercase()\n        ])\n        tokenizer.pre_tokenizer = pre_tokenizers.Metaspace()\n        trainer = trainers.UnigramTrainer(\n            vocab_size=config[\"vocab_size\"], \n            special_tokens=config[\"special_tokens\"], \n            unk_token=\"<unk>\"\n        )\n        tokenizer.train_from_iterator(batch_iterator(dataset, config[\"batch_size\"]), trainer=trainer)\n        cls_id = tokenizer.token_to_id(\"[CLS]\")\n        sep_id = tokenizer.token_to_id(\"[SEP]\")\n        tokenizer.post_processor = processors.TemplateProcessing(\n            single=\"[CLS]:0 $A:0 [SEP]:0\",\n            pair=\"[CLS]:0 $A:0 [SEP]:0 $B:1 [SEP]:1\",\n            special_tokens=[(\"[CLS]\", cls_id), (\"[SEP]\", sep_id)],\n        )\n        tokenizer.decoder = decoders.Metaspace()\n        tokenizer = AlbertTokenizerFast(tokenizer_object=tokenizer)\n\n    else:\n        raise ValueError(f\"Unsupported model type: {model_type}\")\n\n    return tokenizer\n\ndef save_tokenizer(tokenizer, output_dir):\n    tokenizer.save_pretrained(output_dir)\n    logging.info(f\"Tokenizer saved at {output_dir}\")\n\ndef main():\n    dataset = load_data(CONFIG)\n    tokenizer = train_tokenizer_with_model(\"bert\", dataset, CONFIG)  # Change model type as needed\n    save_tokenizer(tokenizer, \"my-custom-tokenizer\")\n\nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-16T16:28:51.741816Z","iopub.execute_input":"2024-12-16T16:28:51.742179Z","iopub.status.idle":"2024-12-16T16:28:59.894677Z","shell.execute_reply.started":"2024-12-16T16:28:51.742120Z","shell.execute_reply":"2024-12-16T16:28:59.893727Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/10.5k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0d7e8008ba1a40f1b5635d52d1f4b6ed"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"test-00000-of-00001.parquet:   0%|          | 0.00/733k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d5464429a35c420bbfdfb0707c624c2f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train-00000-of-00001.parquet:   0%|          | 0.00/6.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"305623727bd04705916ee6440fc9128a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"validation-00000-of-00001.parquet:   0%|          | 0.00/657k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9182c1235a69438081c9fb521eab3c93"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/4358 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2d576b54bfc6422c93a7ac2d0a9a4fe7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/36718 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"428e8a4be834466ebcc5765e82d748b3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split:   0%|          | 0/3760 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bbeb039166cc4793b81b8c6a9c10ac73"}},"metadata":{}},{"name":"stdout","text":"\n\n\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"from datasets import load_dataset\n\ndataset = load_dataset(\"wikitext\", name=\"wikitext-2-raw-v1\")\nprint(dataset[\"train\"][:5])  # Display the first 5 training samples","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-16T16:30:23.822971Z","iopub.execute_input":"2024-12-16T16:30:23.823802Z","iopub.status.idle":"2024-12-16T16:30:27.409926Z","shell.execute_reply.started":"2024-12-16T16:30:23.823768Z","shell.execute_reply":"2024-12-16T16:30:27.409023Z"}},"outputs":[{"name":"stdout","text":"{'text': ['', ' = Valkyria Chronicles III = \\n', '', ' Senjō no Valkyria 3 : Unrecorded Chronicles ( Japanese : 戦場のヴァルキュリア3 , lit . Valkyria of the Battlefield 3 ) , commonly referred to as Valkyria Chronicles III outside Japan , is a tactical role @-@ playing video game developed by Sega and Media.Vision for the PlayStation Portable . Released in January 2011 in Japan , it is the third game in the Valkyria series . Employing the same fusion of tactical and real @-@ time gameplay as its predecessors , the story runs parallel to the first game and follows the \" Nameless \" , a penal military unit serving the nation of Gallia during the Second Europan War who perform secret black operations and are pitted against the Imperial unit \" Calamaty Raven \" . \\n', \" The game began development in 2010 , carrying over a large portion of the work done on Valkyria Chronicles II . While it retained the standard features of the series , it also underwent multiple adjustments , such as making the game more forgiving for series newcomers . Character designer Raita Honjou and composer Hitoshi Sakimoto both returned from previous entries , along with Valkyria Chronicles II director Takeshi Ozawa . A large team of writers handled the script . The game 's opening theme was sung by May 'n . \\n\"]}\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"def batch_iterator(batch_size=1000):\n    for i in range(0, len(dataset[\"train\"]), batch_size):\n        yield dataset[\"train\"][i: i + batch_size][\"text\"]\n\n\nfrom tokenizers import Tokenizer, models, normalizers, pre_tokenizers, trainers, processors\n\n# Initialize the WordPiece tokenizer\ntokenizer = Tokenizer(models.WordPiece(unk_token=\"[UNK]\"))\n\n# Add normalization\ntokenizer.normalizer = normalizers.BertNormalizer(lowercase=True)\n\n# Add pre-tokenization (split inputs into words)\ntokenizer.pre_tokenizer = pre_tokenizers.BertPreTokenizer()\n\n# Special tokens and trainer configuration\nspecial_tokens = [\"[UNK]\", \"[PAD]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"]\ntrainer = trainers.WordPieceTrainer(vocab_size=25000, special_tokens=special_tokens)\n\n# Train the tokenizer on the dataset\ntokenizer.train_from_iterator(batch_iterator(), trainer=trainer)\n\n# Add post-processing for [CLS] and [SEP] tokens\ncls_id = tokenizer.token_to_id(\"[CLS]\")\nsep_id = tokenizer.token_to_id(\"[SEP]\")\ntokenizer.post_processor = processors.TemplateProcessing(\n    single=\"[CLS]:0 $A:0 [SEP]:0\",\n    pair=\"[CLS]:0 $A:0 [SEP]:0 $B:1 [SEP]:1\",\n    special_tokens=[(\"[CLS]\", cls_id), (\"[SEP]\", sep_id)],\n)\n\n# Save the tokenizer\ntokenizer.save(\"my-wordpiece-tokenizer.json\")\nprint(\"Tokenizer training complete. Saved as 'my-wordpiece-tokenizer.json'\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-16T16:32:19.688562Z","iopub.execute_input":"2024-12-16T16:32:19.688896Z","iopub.status.idle":"2024-12-16T16:32:23.185801Z","shell.execute_reply.started":"2024-12-16T16:32:19.688867Z","shell.execute_reply":"2024-12-16T16:32:23.184931Z"}},"outputs":[{"name":"stdout","text":"\n\n\nTokenizer training complete. Saved as 'my-wordpiece-tokenizer.json'\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"# Load the tokenizer\nfrom tokenizers import Tokenizer\n\ntokenizer = Tokenizer.from_file(\"my-wordpiece-tokenizer.json\")\n\n# Encode some text\nencoding = tokenizer.encode(\"This is a test sentence.\")\nprint(\"Tokens:\", encoding.tokens)  # Check the tokenized output\nprint(\"IDs:\", encoding.ids)  # Check the token IDs\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-16T16:32:46.277778Z","iopub.execute_input":"2024-12-16T16:32:46.278115Z","iopub.status.idle":"2024-12-16T16:32:46.306291Z","shell.execute_reply.started":"2024-12-16T16:32:46.278087Z","shell.execute_reply":"2024-12-16T16:32:46.305320Z"}},"outputs":[{"name":"stdout","text":"Tokens: ['[CLS]', 'this', 'is', 'a', 'test', 'sentence', '.', '[SEP]']\nIDs: [2, 1309, 1188, 43, 3395, 6026, 18, 3]\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"from transformers import BertTokenizerFast\n\nhf_tokenizer = BertTokenizerFast(tokenizer_object=tokenizer)\n\n# Save it for reuse\nhf_tokenizer.save_pretrained(\"my-wordpiece-tokenizer\")\n\n# Reload it for later use\nreloaded_tokenizer = BertTokenizerFast.from_pretrained(\"my-wordpiece-tokenizer\")\nprint(reloaded_tokenizer(\"This is a test sentence.\"))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-16T16:32:57.692840Z","iopub.execute_input":"2024-12-16T16:32:57.693508Z","iopub.status.idle":"2024-12-16T16:32:57.747039Z","shell.execute_reply.started":"2024-12-16T16:32:57.693475Z","shell.execute_reply":"2024-12-16T16:32:57.746181Z"}},"outputs":[{"name":"stdout","text":"{'input_ids': [2, 1309, 1188, 43, 3395, 6026, 18, 3], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1]}\n","output_type":"stream"}],"execution_count":10}]}