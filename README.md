# CustomTokenizerProject
training and using a tokenizer, specifically with WordPiece models and datasets like WikiText

## Features
- Custom WordPiece tokenizer training with adjustable vocabulary size.
- Integration with Hugging Face for downstream NLP tasks.
- Easy-to-follow implementation using Python and Tokenizers library.

## How to Use
1. Clone the repository.
2. Install dependencies: `pip install -r requirements.txt`.
3. Run the Jupyter Notebook `training-a-tokenizer.ipynb`.

## Dependencies
- `transformers`
- `datasets`
- `tokenizers`
- `logging`

## Directory Structure
- `training-a-tokenizer.ipynb`: Main notebook for training the tokenizer.
- `tokenizer/`: Contains saved tokenizer files for reuse.
- `requirements.txt`: Dependencies for running the project.
